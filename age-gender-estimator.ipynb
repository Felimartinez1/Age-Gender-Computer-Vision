{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-26T17:47:44.208502Z","iopub.execute_input":"2023-09-26T17:47:44.208893Z","iopub.status.idle":"2023-09-26T17:47:44.242630Z","shell.execute_reply.started":"2023-09-26T17:47:44.208862Z","shell.execute_reply":"2023-09-26T17:47:44.241388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Challenges\n\nIn this project, we aim to address the following challenges:\n\n1. **Image Preprocessing:** Preprocessing the facial images is crucial for the success of the model. This includes resizing, normalizing, and converting images to a format suitable for neural networks. Ensuring the input data is appropriately prepared is essential.\n\n2. **Model Architecture:** Designing an effective neural network architecture that can handle both age regression and gender classification tasks simultaneously is a non-trivial task. Balancing the model's complexity while maintaining good performance is essential.\n\n3. **Gender Classification:** Another significant challenge is to classify the gender of a person from their facial image. The model must learn to distinguish between male and female characteristics, often requiring subtle visual cues.\n\n4. **Age Estimation:** The primary challenge is to develop a model that can accurately estimate the age of a person based on their facial features. This involves training a deep learning model to regress the age of individuals, which can be a complex and nuanced task.\n\n5. **Hyperparameter Tuning:** Finding the right set of hyperparameters for training the model can significantly impact its performance. It involves optimizing learning rates, batch sizes, regularization techniques, and more.\n\n6. **Evaluation Metrics:** Choosing appropriate evaluation metrics for age estimation and gender classification is vital. Mean Absolute Error (MAE) for age regression and accuracy for gender classification are common metrics, but others may be considered.\n\n7. **Data Quality and Quantity:** The quality and quantity of the dataset play a significant role in the model's performance. Ensuring a diverse and representative dataset can be challenging, and data augmentation techniques may be required.\n\n8. **Interpreting Model Predictions:** Understanding how the model arrives at its predictions is crucial, especially in applications like age estimation and gender classification. Visualizing model explanations and uncertainty can be a challenge.\n\nBy addressing these challenges, we aim to create a robust and accurate system for estimating the age and gender of individuals from facial images.\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install seaborn\n!pip install Pillow\n!pip install imgaug\n!pip install opencv-python\n!apt-get update\n!apt-get install -y libgl1-mesa-glx","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:00:48.597913Z","iopub.execute_input":"2023-10-01T23:00:48.598239Z","iopub.status.idle":"2023-10-01T23:01:33.222492Z","shell.execute_reply.started":"2023-10-01T23:00:48.598213Z","shell.execute_reply":"2023-10-01T23:01:33.221258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import load_img\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport random\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nimport imgaug.augmenters as iaa\n\nimport os\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:01:33.224568Z","iopub.execute_input":"2023-10-01T23:01:33.224926Z","iopub.status.idle":"2023-10-01T23:02:15.695584Z","shell.execute_reply.started":"2023-10-01T23:01:33.224898Z","shell.execute_reply":"2023-10-01T23:02:15.694255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configura la estrategia de la TPU\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:15.697036Z","iopub.execute_input":"2023-10-01T23:02:15.697871Z","iopub.status.idle":"2023-10-01T23:02:25.443931Z","shell.execute_reply.started":"2023-10-01T23:02:15.697834Z","shell.execute_reply":"2023-10-01T23:02:25.442835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Labeling","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/utkface-new/UTKFace'\nage_labels = []\ngender_labels = []\nimage_paths = []\n\nimage_filenames = os.listdir(path)\nrandom.shuffle(image_filenames)\n\nfor image in image_filenames:\n    image_path = os.path.join(path, image)\n    img_components = image.split('_')\n    age_label = int(img_components[0])\n    gender_label = int(img_components[1])\n    \n    age_labels.append(age_label)\n    gender_labels.append(gender_label)\n    image_paths.append(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:25.445949Z","iopub.execute_input":"2023-10-01T23:02:25.446200Z","iopub.status.idle":"2023-10-01T23:02:29.527751Z","shell.execute_reply.started":"2023-10-01T23:02:25.446176Z","shell.execute_reply":"2023-10-01T23:02:29.526560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of age_labels: {len(age_labels)}, Number of gender_labels: {len(gender_labels)}, Number of image_paths: {len(image_paths)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:29.528979Z","iopub.execute_input":"2023-10-01T23:02:29.529268Z","iopub.status.idle":"2023-10-01T23:02:29.534461Z","shell.execute_reply.started":"2023-10-01T23:02:29.529243Z","shell.execute_reply":"2023-10-01T23:02:29.533516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(age_labels[:10])\nprint(gender_labels[:10])\nprint(image_paths[:10])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:29.535540Z","iopub.execute_input":"2023-10-01T23:02:29.535808Z","iopub.status.idle":"2023-10-01T23:02:29.547140Z","shell.execute_reply.started":"2023-10-01T23:02:29.535785Z","shell.execute_reply":"2023-10-01T23:02:29.546273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['image_path'], df['age'], df['gender'] = image_paths, age_labels, gender_labels\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:29.548390Z","iopub.execute_input":"2023-10-01T23:02:29.548674Z","iopub.status.idle":"2023-10-01T23:02:29.586668Z","shell.execute_reply.started":"2023-10-01T23:02:29.548650Z","shell.execute_reply":"2023-10-01T23:02:29.585600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributions","metadata":{}},{"cell_type":"code","source":" sns.distplot(df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:29.587844Z","iopub.execute_input":"2023-10-01T23:02:29.588130Z","iopub.status.idle":"2023-10-01T23:02:30.109335Z","shell.execute_reply.started":"2023-10-01T23:02:29.588103Z","shell.execute_reply":"2023-10-01T23:02:30.108289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x='gender')\n\n# Añade etiquetas y título al gráfico\nplt.xlabel('Gender')\nplt.ylabel('Quantity')\nplt.title('Gender Distribution')\n\n# Muestra el gráfico\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:30.110415Z","iopub.execute_input":"2023-10-01T23:02:30.110696Z","iopub.status.idle":"2023-10-01T23:02:30.326573Z","shell.execute_reply.started":"2023-10-01T23:02:30.110670Z","shell.execute_reply":"2023-10-01T23:02:30.325668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"def apply_data_augmentation(image_path):\n    # Carga la imagen\n    img = load_img(image_path)\n    img = img_to_array(img)\n    \n    # Define una secuencia de aumentos de datos\n    seq = iaa.Sequential([\n        iaa.Affine(rotate=(-10, 10)),  # Rotación en un rango de -10 a 10 grados\n        iaa.Fliplr(0.9),  # Volteo horizontal con probabilidad del 90%\n        iaa.Sometimes(0.7, iaa.GaussianBlur(sigma=(0, 2.0))),  # Aplicar desenfoque gaussiano con probabilidad del 70%\n        iaa.Sometimes(0.6, iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255))),  # Agregar ruido gaussiano con probabilidad del 60%\n        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),  # Ajustar el contraste\n    ])\n    \n    # Aplica las transformaciones\n    augmented_img = seq(image=img)\n    return augmented_img\n\n# Selecciona las imágenes de edades entre 5 a 20 años y de 40 a 90 años\nselected_images = df[(df['age'] >= 5) & (df['age'] <= 20) | (df['age'] >= 40) & (df['age'] <= 90)]\n\n# Aplica aumentos de datos a las imágenes seleccionadas\naugmented_images = []\nfor image_path in selected_images['image_path']:\n    augmented_img = apply_data_augmentation(image_path)\n    augmented_images.append(augmented_img)\n\n# Crear una nueva lista de rutas de imagen para las imágenes aumentadas\naugmented_image_paths = []\n\n# Crear una nueva lista de edades y géneros para las imágenes aumentadas\naugmented_age_labels = []\naugmented_gender_labels = []\n\n# Directorio donde guardar las imágenes aumentadas\n\n############\noutput_directory = '/kaggle/working/augmented_images'  ##### Cambia esto a la ubicación deseada\nos.makedirs(output_directory, exist_ok=True)\n############\n\n# Iterar a través de las imágenes originales y sus correspondientes imágenes aumentadas\nfor image_path, age_label, gender_label, augmented_img in zip(selected_images['image_path'], selected_images['age'], selected_images['gender'], augmented_images):\n    # Generar una nueva ruta de imagen para la imagen aumentada\n    augmented_image_filename = os.path.basename(image_path).replace('.jpg', '_augmented.jpg')\n    augmented_image_path = os.path.join(output_directory, augmented_image_filename)\n    \n    # Guardar la ruta de imagen aumentada en la lista\n    augmented_image_paths.append(augmented_image_path)\n    \n    # Guardar la misma edad y género para la imagen aumentada\n    augmented_age_labels.append(age_label)\n    augmented_gender_labels.append(gender_label)\n    \n    # Guardar la imagen aumentada en el sistema de archivos\n    plt.imsave(augmented_image_path, augmented_img.astype(np.uint8))\n\n# Verifica el número total de imágenes originales y aumentadas\nprint(f\"Número de imágenes originales: {len(selected_images)}\")\nprint(f\"Número de imágenes aumentadas: {len(augmented_images)}\")\n\n# Crear un nuevo DataFrame con las imágenes aumentadas y sus etiquetas\naugmented_df = pd.DataFrame()\naugmented_df['image_path'] = augmented_image_paths\naugmented_df['age'] = augmented_age_labels\naugmented_df['gender'] = augmented_gender_labels","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:02:30.329698Z","iopub.execute_input":"2023-10-01T23:02:30.330014Z","iopub.status.idle":"2023-10-01T23:07:22.562331Z","shell.execute_reply.started":"2023-10-01T23:02:30.329988Z","shell.execute_reply":"2023-10-01T23:07:22.561270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([df, augmented_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:22.563575Z","iopub.execute_input":"2023-10-01T23:07:22.563904Z","iopub.status.idle":"2023-10-01T23:07:22.570023Z","shell.execute_reply.started":"2023-10-01T23:07:22.563875Z","shell.execute_reply":"2023-10-01T23:07:22.569140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:22.571095Z","iopub.execute_input":"2023-10-01T23:07:22.571881Z","iopub.status.idle":"2023-10-01T23:07:22.588453Z","shell.execute_reply.started":"2023-10-01T23:07:22.571851Z","shell.execute_reply":"2023-10-01T23:07:22.587561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **BEFORE**","metadata":{}},{"cell_type":"code","source":"sns.distplot(df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:22.589510Z","iopub.execute_input":"2023-10-01T23:07:22.589835Z","iopub.status.idle":"2023-10-01T23:07:23.082803Z","shell.execute_reply.started":"2023-10-01T23:07:22.589809Z","shell.execute_reply":"2023-10-01T23:07:23.081763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **AFTER**","metadata":{}},{"cell_type":"code","source":"sns.distplot(combined_df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:23.084089Z","iopub.execute_input":"2023-10-01T23:07:23.084431Z","iopub.status.idle":"2023-10-01T23:07:23.580224Z","shell.execute_reply.started":"2023-10-01T23:07:23.084401Z","shell.execute_reply":"2023-10-01T23:07:23.579363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualiza una imagen original y su versión aumentada\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.title(\"Original Image\")\nimg = load_img(selected_images.iloc[3]['image_path'])\nplt.imshow(img)\nplt.subplot(1, 2, 2)\nplt.title(\"Augmented Image\")\naugmented_img = augmented_images[3]\naugmented_img = np.clip(augmented_img, 0, 255).astype(np.uint8)  # Normaliza los valores de píxeles\nplt.imshow(augmented_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:23.581353Z","iopub.execute_input":"2023-10-01T23:07:23.581652Z","iopub.status.idle":"2023-10-01T23:07:23.965124Z","shell.execute_reply.started":"2023-10-01T23:07:23.581610Z","shell.execute_reply":"2023-10-01T23:07:23.964184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def extract_image_features(images):\n    features = list()\n\n    for image in images:\n        img = load_img(image, grayscale=True)\n        img = img.resize((128, 128), Image.LANCZOS)\n        img = np.array(img)\n        features.append(img)\n\n    features = np.array(features)\n    features = features.reshape(len(features), 128, 128, 1)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:23.966177Z","iopub.execute_input":"2023-10-01T23:07:23.966449Z","iopub.status.idle":"2023-10-01T23:07:23.971540Z","shell.execute_reply.started":"2023-10-01T23:07:23.966425Z","shell.execute_reply":"2023-10-01T23:07:23.970810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtra el DataFrame para obtener solo las imágenes con edad menor a 70\nfiltered_df = combined_df[combined_df['age'] < 50]\n\n# Extrae las características de las imágenes\nX = extract_image_features(filtered_df['image_path'])\n\n# Normaliza las características\nX = X / 255.0\n\n# Verifica la forma de X\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:07:23.972443Z","iopub.execute_input":"2023-10-01T23:07:23.972710Z","iopub.status.idle":"2023-10-01T23:13:03.077812Z","shell.execute_reply.started":"2023-10-01T23:07:23.972672Z","shell.execute_reply":"2023-10-01T23:13:03.076844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_gender = np.array(filtered_df['gender'])\ny_age = np.array(filtered_df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:13:03.078974Z","iopub.execute_input":"2023-10-01T23:13:03.079252Z","iopub.status.idle":"2023-10-01T23:13:03.084102Z","shell.execute_reply.started":"2023-10-01T23:13:03.079228Z","shell.execute_reply":"2023-10-01T23:13:03.083249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Duplica el canal de imágenes en escala de grises para convertirlas en imágenes RGB\nX_rgb = np.repeat(X, 3, axis=-1)\n\n# Asegúrate de que las imágenes estén en el rango [0, 255]\nX_rgb = X_rgb * 255\n\n# Redimensiona tus imágenes a 224x224\ndef resize_images(images):\n    resized_images = []\n    for image in images:\n        img = cv2.resize(image, (224, 224))\n        resized_images.append(img)\n    return np.array(resized_images)\n\nX_rgb = resize_images(X_rgb)\n\n# Asegúrate de que las imágenes estén en el rango [0, 1]\nX_rgb = X_rgb / 255.0\n\n# Divide tus datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_gender_train, y_gender_test, y_age_train, y_age_test = train_test_split(X_rgb, y_gender, y_age, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T23:13:03.085110Z","iopub.execute_input":"2023-10-01T23:13:03.085341Z","iopub.status.idle":"2023-10-01T23:14:06.075020Z","shell.execute_reply.started":"2023-10-01T23:13:03.085321Z","shell.execute_reply":"2023-10-01T23:14:06.073802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building and fitting models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# Definir la entrada de la red\ninput_shape = (224, 224, 3)\n\nwith strategy.scope():\n    # Define el modelo\n    inputs = Input(shape=input_shape)\n\n    # Capas de convolución y max-pooling\n    conv_1 = Conv2D(64, kernel_size=(3, 3), activation='relu')(inputs)\n    max_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n    conv_2 = Conv2D(128, kernel_size=(3, 3), activation='relu')(max_1)\n    max_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n    conv_3 = Conv2D(256, kernel_size=(3, 3), activation='relu')(max_2)\n    max_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n    conv_4 = Conv2D(512, kernel_size=(3, 3), activation='relu')(max_3)\n    max_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n\n    # Capas de aplanamiento y completamente conectadas\n    flatten = Flatten()(max_4)\n    dense_1 = Dense(512, activation='relu')(flatten)\n    dropout_1 = Dropout(0.5)(dense_1)\n    dense_2 = Dense(256, activation='relu')(dropout_1)\n    dropout_2 = Dropout(0.5)(dense_2)\n\n    # Capa de salida para la predicción de género\n    output_gender = Dense(1, activation='sigmoid', name='gender_out')(dropout_2)\n\n    # Crear el modelo\n    gender_model = Model(inputs=inputs, outputs=output_gender)\n\n    # Compilar el modelo solo para la predicción de género\n    gender_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n\n\n    \n\n    inputs = Input(shape=input_shape)\n\n    # Convolution and max-pooling layers\n    conv_1 = Conv2D(64, kernel_size=(3, 3), activation='relu')(inputs)\n    max_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n    conv_2 = Conv2D(128, kernel_size=(3, 3), activation='relu')(max_1)\n    max_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n    conv_3 = Conv2D(256, kernel_size=(3, 3), activation='relu')(max_2)\n    max_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n    conv_4 = Conv2D(512, kernel_size=(3, 3), activation='relu')(max_3)\n    max_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n\n    # Flattening and fully connected layers\n    flatten = Flatten()(max_4)\n    dense_1 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n    dropout_1 = Dropout(0.5)(dense_1)\n    dense_2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(dropout_1)\n    dropout_2 = Dropout(0.3)(dense_2)\n    dense_3 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n    output_age = Dense(1, activation='linear', name='age_out')(dense_3)\n\n    # Create and compile the age model\n    age_model = Model(inputs=inputs, outputs=output_age)\n    age_model.compile(loss='mean_absolute_error', optimizer='RMSprop', metrics=['mae'])\n\n    # Entrena ambos modelos por separado\n    gender_model.fit(X_train, y_gender_train, epochs=20, batch_size=32, validation_split=0.2)\n    age_model.fit(X_train, y_age_train, epochs=20, batch_size=32, validation_split=0.2)\n\n    # Evalúa ambos modelos\n    gender_loss, gender_accuracy = gender_model.evaluate(X_test, y_gender_test)\n    age_loss, age_mae = age_model.evaluate(X_test, y_age_test)\n\nprint(\"Gender Model - Loss:\", gender_loss, \"Accuracy:\", gender_accuracy)\nprint(\"Age Model - Loss:\", age_loss, \"MAE:\", age_mae)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T01:06:06.884786Z","iopub.execute_input":"2023-10-02T01:06:06.885220Z","iopub.status.idle":"2023-10-02T01:18:45.264036Z","shell.execute_reply.started":"2023-10-02T01:06:06.885188Z","shell.execute_reply":"2023-10-02T01:18:45.262695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"# Carga una imagen de prueba\nimg_path = '/kaggle/input/lautaro/lautaro.jpg'  # Reemplaza con la ruta de tu propia imagen\nimg = cv2.imread(img_path)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Utiliza un detector de caras para obtener las coordenadas de la bounding box\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\nfor (x, y, w, h) in faces:\n    # Recorta la región de interés (ROI) de la cara\n    face_roi = img[y:y+h, x:x+w]\n    \n    # Preprocesa la imagen\n    face_roi = cv2.resize(face_roi, (224, 224))\n    face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n    face_roi = face_roi.astype(np.float32) / 255.0\n\n    # Realiza la predicción de género\n    gender_prediction = gender_model.predict(np.expand_dims(face_roi, axis=0))\n    gender_label = \"Man\" if gender_prediction < 0.5 else \"Women\"\n\n    # Realiza la predicción de edad\n    age_prediction = age_model.predict(np.expand_dims(face_roi, axis=0))\n\n    # Dibuja la bounding box en la imagen original\n    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n    # Muestra las predicciones en la imagen\n    label = f'Gender: {gender_label}, Age: {int(age_prediction[0][0])}'\n    cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Muestra la imagen con bounding boxes y predicciones\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:02:53.268916Z","iopub.execute_input":"2023-10-02T02:02:53.269341Z","iopub.status.idle":"2023-10-02T02:02:54.673583Z","shell.execute_reply.started":"2023-10-02T02:02:53.269309Z","shell.execute_reply":"2023-10-02T02:02:54.672368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}