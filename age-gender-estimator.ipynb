{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Challenges\n\nIn this project, we aim to address the following challenges:\n\n1. **Image Preprocessing:** Preprocessing the facial images is crucial for the success of the model. This includes resizing, normalizing, and converting images to a format suitable for neural networks. Ensuring the input data is appropriately prepared is essential.\n\n2. **Model Architecture:** Designing an effective neural network architecture that can handle both age regression and gender classification tasks simultaneously is a non-trivial task. Balancing the model's complexity while maintaining good performance is essential.\n\n3. **Gender Classification:** Another significant challenge is to classify the gender of a person from their facial image. The model must learn to distinguish between male and female characteristics, often requiring subtle visual cues.\n\n4. **Age Estimation:** The primary challenge is to develop a model that can accurately estimate the age of a person based on their facial features. This involves training a deep learning model to regress the age of individuals, which can be a complex and nuanced task.\n\n5. **Hyperparameter Tuning:** Finding the right set of hyperparameters for training the model can significantly impact its performance. It involves optimizing learning rates, batch sizes, regularization techniques, and more.\n\n6. **Evaluation Metrics:** Choosing appropriate evaluation metrics for age estimation and gender classification is vital. Mean Absolute Error (MAE) for age regression and accuracy for gender classification are common metrics, but others may be considered.\n\n7. **Data Quality and Quantity:** The quality and quantity of the dataset play a significant role in the model's performance. Ensuring a diverse and representative dataset can be challenging, and data augmentation techniques may be required.\n\n8. **Interpreting Model Predictions:** Understanding how the model arrives at its predictions is crucial, especially in applications like age estimation and gender classification. Visualizing model explanations and uncertainty can be a challenge.\n\nBy addressing these challenges, we aim to create a robust and accurate system for estimating the age and gender of individuals from facial images.\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install seaborn\n!pip install Pillow\n!pip install imgaug\n!pip install opencv-python\n!apt-get update\n!apt-get install -y libgl1-mesa-glx","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:39:06.319634Z","iopub.execute_input":"2023-10-03T14:39:06.320062Z","iopub.status.idle":"2023-10-03T14:39:52.685571Z","shell.execute_reply.started":"2023-10-03T14:39:06.320031Z","shell.execute_reply":"2023-10-03T14:39:52.684442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import load_img\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Input, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nimport random\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\nimport imgaug.augmenters as iaa\n\nimport os\nfrom tqdm.notebook import tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:39:52.687249Z","iopub.execute_input":"2023-10-03T14:39:52.687522Z","iopub.status.idle":"2023-10-03T14:40:35.722635Z","shell.execute_reply.started":"2023-10-03T14:39:52.687496Z","shell.execute_reply":"2023-10-03T14:40:35.721667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configura la estrategia de la TPU\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:35.723753Z","iopub.execute_input":"2023-10-03T14:40:35.724423Z","iopub.status.idle":"2023-10-03T14:40:44.549945Z","shell.execute_reply.started":"2023-10-03T14:40:35.724392Z","shell.execute_reply":"2023-10-03T14:40:44.549061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Labeling","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/utkface-new/UTKFace'\nage_labels = []\ngender_labels = []\nimage_paths = []\n\nimage_filenames = os.listdir(path)\nrandom.shuffle(image_filenames)\n\nfor image in image_filenames:\n    image_path = os.path.join(path, image)\n    img_components = image.split('_')\n    age_label = int(img_components[0])\n    gender_label = int(img_components[1])\n    \n    age_labels.append(age_label)\n    gender_labels.append(gender_label)\n    image_paths.append(image_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:44.551888Z","iopub.execute_input":"2023-10-03T14:40:44.552173Z","iopub.status.idle":"2023-10-03T14:40:45.029752Z","shell.execute_reply.started":"2023-10-03T14:40:44.552144Z","shell.execute_reply":"2023-10-03T14:40:45.028646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Number of age_labels: {len(age_labels)}, Number of gender_labels: {len(gender_labels)}, Number of image_paths: {len(image_paths)}')","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.030913Z","iopub.execute_input":"2023-10-03T14:40:45.031188Z","iopub.status.idle":"2023-10-03T14:40:45.037414Z","shell.execute_reply.started":"2023-10-03T14:40:45.031162Z","shell.execute_reply":"2023-10-03T14:40:45.036007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(age_labels[:10])\nprint(gender_labels[:10])\nprint(image_paths[:10])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.038813Z","iopub.execute_input":"2023-10-03T14:40:45.039104Z","iopub.status.idle":"2023-10-03T14:40:45.075012Z","shell.execute_reply.started":"2023-10-03T14:40:45.039079Z","shell.execute_reply":"2023-10-03T14:40:45.073973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame()\ndf['image_path'], df['age'], df['gender'] = image_paths, age_labels, gender_labels\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.076453Z","iopub.execute_input":"2023-10-03T14:40:45.076824Z","iopub.status.idle":"2023-10-03T14:40:45.129082Z","shell.execute_reply.started":"2023-10-03T14:40:45.076784Z","shell.execute_reply":"2023-10-03T14:40:45.128082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Distributions","metadata":{}},{"cell_type":"code","source":" sns.distplot(df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.130414Z","iopub.execute_input":"2023-10-03T14:40:45.130686Z","iopub.status.idle":"2023-10-03T14:40:45.642624Z","shell.execute_reply.started":"2023-10-03T14:40:45.130661Z","shell.execute_reply":"2023-10-03T14:40:45.641595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(data=df, x='gender')\n\nplt.xlabel('Gender')\nplt.ylabel('Quantity')\nplt.title('Gender Distribution')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.643800Z","iopub.execute_input":"2023-10-03T14:40:45.644111Z","iopub.status.idle":"2023-10-03T14:40:45.863170Z","shell.execute_reply.started":"2023-10-03T14:40:45.644082Z","shell.execute_reply":"2023-10-03T14:40:45.862042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation","metadata":{}},{"cell_type":"code","source":"def apply_data_augmentation(image_path):\n    img = load_img(image_path)\n    img = img_to_array(img)\n    \n    seq = iaa.Sequential([\n        iaa.Affine(rotate=(-10, 10)),  \n        iaa.Fliplr(0.9),  \n        iaa.Sometimes(0.7, iaa.GaussianBlur(sigma=(0, 2.0))), \n        iaa.Sometimes(0.6, iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255))),  \n        iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),  \n    ])\n    \n    augmented_img = seq(image=img)\n    return augmented_img\n\nselected_images = df[(df['age'] >= 5) & (df['age'] <= 20) | (df['age'] >= 40) & (df['age'] <= 90)]\n\naugmented_images = []\nfor image_path in selected_images['image_path']:\n    augmented_img = apply_data_augmentation(image_path)\n    augmented_images.append(augmented_img)\n\naugmented_image_paths = []\n\naugmented_age_labels = []\naugmented_gender_labels = []\n\n\n############\noutput_directory = '/kaggle/working/augmented_images'  ##### Change this to the desired location\nos.makedirs(output_directory, exist_ok=True)\n############\n\nfor image_path, age_label, gender_label, augmented_img in zip(selected_images['image_path'], selected_images['age'], selected_images['gender'], augmented_images):\n    augmented_image_filename = os.path.basename(image_path).replace('.jpg', '_augmented.jpg')\n    augmented_image_path = os.path.join(output_directory, augmented_image_filename)\n    \n    augmented_image_paths.append(augmented_image_path)\n    \n    augmented_age_labels.append(age_label)\n    augmented_gender_labels.append(gender_label)\n    \n    plt.imsave(augmented_image_path, augmented_img.astype(np.uint8))\n\nprint(f\"Número de imágenes originales: {len(selected_images)}\")\nprint(f\"Número de imágenes aumentadas: {len(augmented_images)}\")\n\naugmented_df = pd.DataFrame()\naugmented_df['image_path'] = augmented_image_paths\naugmented_df['age'] = augmented_age_labels\naugmented_df['gender'] = augmented_gender_labels","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:40:45.866214Z","iopub.execute_input":"2023-10-03T14:40:45.866505Z","iopub.status.idle":"2023-10-03T14:42:57.385325Z","shell.execute_reply.started":"2023-10-03T14:40:45.866480Z","shell.execute_reply":"2023-10-03T14:42:57.384421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([df, augmented_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:57.386420Z","iopub.execute_input":"2023-10-03T14:42:57.386661Z","iopub.status.idle":"2023-10-03T14:42:57.391997Z","shell.execute_reply.started":"2023-10-03T14:42:57.386637Z","shell.execute_reply":"2023-10-03T14:42:57.391324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:57.392846Z","iopub.execute_input":"2023-10-03T14:42:57.393079Z","iopub.status.idle":"2023-10-03T14:42:57.414767Z","shell.execute_reply.started":"2023-10-03T14:42:57.393059Z","shell.execute_reply":"2023-10-03T14:42:57.414045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **BEFORE**","metadata":{}},{"cell_type":"code","source":"sns.distplot(df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:57.415629Z","iopub.execute_input":"2023-10-03T14:42:57.415870Z","iopub.status.idle":"2023-10-03T14:42:57.876996Z","shell.execute_reply.started":"2023-10-03T14:42:57.415848Z","shell.execute_reply":"2023-10-03T14:42:57.876121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **AFTER**","metadata":{}},{"cell_type":"code","source":"sns.distplot(combined_df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:57.878044Z","iopub.execute_input":"2023-10-03T14:42:57.878332Z","iopub.status.idle":"2023-10-03T14:42:58.330936Z","shell.execute_reply.started":"2023-10-03T14:42:57.878305Z","shell.execute_reply":"2023-10-03T14:42:58.330183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.title(\"Original Image\")\nimg = load_img(selected_images.iloc[3]['image_path'])\nplt.imshow(img)\nplt.subplot(1, 2, 2)\nplt.title(\"Augmented Image\")\naugmented_img = augmented_images[3]\naugmented_img = np.clip(augmented_img, 0, 255).astype(np.uint8)  \nplt.imshow(augmented_img)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:58.331979Z","iopub.execute_input":"2023-10-03T14:42:58.332238Z","iopub.status.idle":"2023-10-03T14:42:58.765260Z","shell.execute_reply.started":"2023-10-03T14:42:58.332208Z","shell.execute_reply":"2023-10-03T14:42:58.764422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def extract_image_features(images):\n    features = list()\n\n    for image in images:\n        img = load_img(image, grayscale=True)\n        img = img.resize((128, 128), Image.LANCZOS)\n        img = np.array(img)\n        features.append(img)\n\n    features = np.array(features)\n    features = features.reshape(len(features), 128, 128, 1)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:58.766230Z","iopub.execute_input":"2023-10-03T14:42:58.766471Z","iopub.status.idle":"2023-10-03T14:42:58.771815Z","shell.execute_reply.started":"2023-10-03T14:42:58.766447Z","shell.execute_reply":"2023-10-03T14:42:58.771041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_df = combined_df[combined_df['age'] < 60]\n\nX = extract_image_features(filtered_df['image_path'])\n\nX = X / 255.0\n\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:42:58.772749Z","iopub.execute_input":"2023-10-03T14:42:58.773014Z","iopub.status.idle":"2023-10-03T14:44:55.426726Z","shell.execute_reply.started":"2023-10-03T14:42:58.772990Z","shell.execute_reply":"2023-10-03T14:44:55.425807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_gender = np.array(filtered_df['gender'])\ny_age = np.array(filtered_df['age'])","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:44:55.428030Z","iopub.execute_input":"2023-10-03T14:44:55.428338Z","iopub.status.idle":"2023-10-03T14:44:55.432912Z","shell.execute_reply.started":"2023-10-03T14:44:55.428311Z","shell.execute_reply":"2023-10-03T14:44:55.432161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_rgb = np.repeat(X, 3, axis=-1)\n\nX_rgb = X_rgb * 255\n\ndef resize_images(images):\n    resized_images = []\n    for image in images:\n        img = cv2.resize(image, (224, 224))\n        resized_images.append(img)\n    return np.array(resized_images)\n\nX_rgb = resize_images(X_rgb)\n\nX_rgb = X_rgb / 255.0\n\nX_train, X_test, y_gender_train, y_gender_test, y_age_train, y_age_test = train_test_split(X_rgb, y_gender, y_age, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:44:55.433878Z","iopub.execute_input":"2023-10-03T14:44:55.434115Z","iopub.status.idle":"2023-10-03T14:46:07.120209Z","shell.execute_reply.started":"2023-10-03T14:44:55.434094Z","shell.execute_reply":"2023-10-03T14:46:07.119094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building and fitting models","metadata":{}},{"cell_type":"code","source":"def create_gender_model(learning_rate=0.001, dropout_rate=0.5, optimizer='adam', kernel_size=(3, 3), pool_size=(2, 2)):\n    input_shape = (224, 224, 3)\n    with strategy.scope():\n        inputs = Input(shape=input_shape)\n        \n        conv_1 = Conv2D(64, kernel_size=kernel_size, activation='relu')(inputs)\n        max_1 = MaxPooling2D(pool_size=pool_size)(conv_1)\n        conv_2 = Conv2D(128, kernel_size=kernel_size, activation='relu')(max_1)\n        max_2 = MaxPooling2D(pool_size=pool_size)(conv_2)\n        conv_3 = Conv2D(256, kernel_size=kernel_size, activation='relu')(max_2)\n        max_3 = MaxPooling2D(pool_size=pool_size)(conv_3)\n        conv_4 = Conv2D(512, kernel_size=kernel_size, activation='relu')(max_3)\n        max_4 = MaxPooling2D(pool_size=pool_size)(conv_4)\n\n        flatten = Flatten()(max_4)\n        dense_1 = Dense(512, activation='relu')(flatten)\n        dropout_1 = Dropout(dropout_rate)(dense_1)\n        dense_2 = Dense(256, activation='relu')(dropout_1)\n        dropout_2 = Dropout(dropout_rate)(dense_2)\n        \n        output_gender = Dense(1, activation='sigmoid', name='gender_out')(dropout_2)\n        \n        if optimizer == 'adam':\n            optimizer = Adam(learning_rate=learning_rate)\n        elif optimizer == 'rmsprop':\n            optimizer = RMSprop(learning_rate=learning_rate)\n        elif optimizer == 'sgd':\n            optimizer = SGD(learning_rate=learning_rate)\n            \n        gender_model = Model(inputs=inputs, outputs=output_gender)\n        gender_model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return gender_model\n\ndef create_age_model(learning_rate=0.001, dropout_rate=0.3, optimizer='adam', kernel_size=(3, 3), pool_size=(2, 2)):\n    input_shape = (224, 224, 3)\n    with strategy.scope():\n        inputs = Input(shape=input_shape)\n        \n        conv_1 = Conv2D(64, kernel_size=kernel_size, activation='relu')(inputs)\n        max_1 = MaxPooling2D(pool_size=pool_size)(conv_1)\n        conv_2 = Conv2D(128, kernel_size=kernel_size, activation='relu')(max_1)\n        max_2 = MaxPooling2D(pool_size=pool_size)(conv_2)\n        conv_3 = Conv2D(256, kernel_size=kernel_size, activation='relu')(max_2)\n        max_3 = MaxPooling2D(pool_size=pool_size)(conv_3)\n        conv_4 = Conv2D(512, kernel_size=kernel_size, activation='relu')(max_3)\n        max_4 = MaxPooling2D(pool_size=pool_size)(conv_4)\n\n        flatten = Flatten()(max_4)\n        dense_1 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n        dropout_1 = Dropout(dropout_rate)(dense_1)\n        dense_2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(dropout_1)\n        dropout_2 = Dropout(dropout_rate)(dense_2)\n        dense_3 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n        \n        output_age = Dense(1, activation='linear', name='age_out')(dense_3)\n        \n        if optimizer == 'adam':\n            optimizer = Adam(learning_rate=learning_rate)\n        elif optimizer == 'rmsprop':\n            optimizer = RMSprop(learning_rate=learning_rate)\n        elif optimizer == 'sgd':\n            optimizer = SGD(learning_rate=learning_rate)\n        \n        age_model = Model(inputs=inputs, outputs=output_age)\n        age_model.compile(loss='mean_absolute_error', optimizer=optimizer, metrics=['mae'])\n    return age_model\n\ngender_model = KerasClassifier(build_fn=create_gender_model, verbose=0)\nage_model = KerasRegressor(build_fn=create_age_model, verbose=0)\n\nparam_grid_gender = {\n    'learning_rate': [0.0001, 0.00001],\n    'dropout_rate': [0.3, 0.4],\n    'optimizer': ['adam', 'rmsprop'],\n    'kernel_size': [(3, 3), (4, 4)],\n    'pool_size': [(2, 2), (3, 3)]\n}\n\nparam_grid_age = {\n    'learning_rate': [0.0001, 0.00001],\n    'dropout_rate': [0.3, 0.4],\n    'optimizer': ['adam', 'rmsprop'],\n    'kernel_size': [(3, 3), (4, 4)],\n    'pool_size': [(2, 2), (3, 3)]\n}\n\ngrid_gender = GridSearchCV(estimator=gender_model, param_grid=param_grid_gender, cv=3, verbose=2)\ngrid_gender_result = grid_gender.fit(X_train, y_gender_train)\n\ngrid_age = GridSearchCV(estimator=age_model, param_grid=param_grid_age, cv=3, verbose=2)\ngrid_age_result = grid_age.fit(X_train, y_age_train)\n\nprint(\"Mejores Hiperparámetros para Género: \", grid_gender_result.best_params_)\nprint(\"Mejor Puntaje para Género: \", grid_gender_result.best_score_)\n\nprint(\"Mejores Hiperparámetros para Edad: \", grid_age_result.best_params_)\nprint(\"Mejor Puntaje para Edad: \", -grid_age_result.best_score_)  # We use the negative of the score since KerasRegressor minimizes the error","metadata":{"execution":{"iopub.status.busy":"2023-10-03T14:46:07.121657Z","iopub.execute_input":"2023-10-03T14:46:07.121945Z","iopub.status.idle":"2023-10-03T18:13:16.481949Z","shell.execute_reply.started":"2023-10-03T14:46:07.121919Z","shell.execute_reply":"2023-10-03T18:13:16.479928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = (224, 224, 3)\n\nwith strategy.scope():\n\n    inputs = Input(shape=input_shape)\n\n    # Convolution and max-pooling layers\n    conv_1 = Conv2D(64, kernel_size=(4, 4), activation='relu')(inputs)\n    max_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n    conv_2 = Conv2D(128, kernel_size=(4, 4), activation='relu')(max_1)\n    max_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n    conv_3 = Conv2D(256, kernel_size=(4, 4), activation='relu')(max_2)\n    max_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n    conv_4 = Conv2D(512, kernel_size=(4, 4), activation='relu')(max_3)\n    max_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n\n    # Flattening and fully connected layers\n    flatten = Flatten()(max_4)\n    dense_1 = Dense(512, activation='relu')(flatten)\n    dropout_1 = Dropout(0.3)(dense_1)\n    dense_2 = Dense(256, activation='relu')(dropout_1)\n    dropout_2 = Dropout(0.3)(dense_2)\n    output_gender = Dense(1, activation='sigmoid', name='gender_out')(dropout_2)\n\n    gender_model = Model(inputs=inputs, outputs=output_gender)\n    gender_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n\n\n    \n\n    inputs = Input(shape=input_shape)\n\n    # Convolution and max-pooling layers\n    conv_1 = Conv2D(64, kernel_size=(3, 3), activation='relu')(inputs)\n    max_1 = MaxPooling2D(pool_size=(2, 2))(conv_1)\n    conv_2 = Conv2D(128, kernel_size=(3, 3), activation='relu')(max_1)\n    max_2 = MaxPooling2D(pool_size=(2, 2))(conv_2)\n    conv_3 = Conv2D(256, kernel_size=(3, 3), activation='relu')(max_2)\n    max_3 = MaxPooling2D(pool_size=(2, 2))(conv_3)\n    conv_4 = Conv2D(512, kernel_size=(3, 3), activation='relu')(max_3)\n    max_4 = MaxPooling2D(pool_size=(2, 2))(conv_4)\n\n    # Flattening and fully connected layers\n    flatten = Flatten()(max_4)\n    dense_1 = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n    dropout_1 = Dropout(0.3)(dense_1)\n    dense_2 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(dropout_1)\n    dropout_2 = Dropout(0.3)(dense_2)\n    dense_3 = Dense(128, activation='relu', kernel_regularizer=l2(0.01))(dropout_2)\n    output_age = Dense(1, activation='linear', name='age_out')(dense_3)\n\n    age_model = Model(inputs=inputs, outputs=output_age)\n    age_model.compile(loss='mean_absolute_error', optimizer=RMSprop(learning_rate=0.0001), metrics=['mae'])\n\n    \n    \n    \n    gender_model.fit(X_train, y_gender_train, epochs=20, batch_size=32, validation_split=0.2)\n    age_model.fit(X_train, y_age_train, epochs=20, batch_size=32, validation_split=0.2)\n\n    gender_loss, gender_accuracy = gender_model.evaluate(X_test, y_gender_test)\n    age_loss, age_mae = age_model.evaluate(X_test, y_age_test)\n\nprint(\"Gender Model - Loss:\", gender_loss, \"Accuracy:\", gender_accuracy)\nprint(\"Age Model - Loss:\", age_loss, \"MAE:\", age_mae)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-03T18:25:49.958317Z","iopub.execute_input":"2023-10-03T18:25:49.959217Z","iopub.status.idle":"2023-10-03T18:31:33.285436Z","shell.execute_reply.started":"2023-10-03T18:25:49.959179Z","shell.execute_reply":"2023-10-03T18:31:33.283614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"img_path = '/kaggle/input/lautaro/lautaro.jpg' \nimg = cv2.imread(img_path)\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\nfor (x, y, w, h) in faces:\n    face_roi = img[y:y+h, x:x+w]\n    \n    face_roi = cv2.resize(face_roi, (224, 224))\n    face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)\n    face_roi = face_roi.astype(np.float32) / 255.0\n\n    gender_prediction = gender_model.predict(np.expand_dims(face_roi, axis=0))\n    gender_label = \"Man\" if gender_prediction < 0.5 else \"Women\"\n\n    age_prediction = age_model.predict(np.expand_dims(face_roi, axis=0))\n\n    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n    label = f'Gender: {gender_label}, Age: {int(age_prediction[0][0])}'\n    cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\nplt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}